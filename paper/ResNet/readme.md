# ResNet论文记录和代码复现

## Abstract

深層次的神經網路難訓練，提出殘差學習框架來簡化訓練，



之前。 我們明確地將層重新表述為參考層輸入的學習殘差函數，而不是學習未參考函數。







## Introduction

深度網絡以端到端的多層方式自然地集成了低/中/高級特徵[50]和分類器，並且可以豐富特徵的“層次”
按堆疊層數（深度）



深度學習發展過程中，出現了一個問題：是
學習更好的網絡就像堆疊更多層一樣簡單？
回答這個問題的一個障礙是臭名昭著的
**梯度消失/爆炸問題**



這個問題，從一開始就阻礙了收斂，然而這個問題在很大程度上通過歸一化初始化和中間歸一化得到數，層網路開始收斂，隨機梯度開始收斂。



當更深層次的網絡能夠開始收斂時，退化問題已經暴露：隨著網絡層數的增加，準確率開始變得飽和，



> （訓練準確性的）退化表明沒有所有系統都同樣易於優化。 讓我們考慮一個
> 較淺的架構及其更深的對應物
> 更多層在上面。 存在構造解
> 到更深的模型：添加的層是恆等映射，
> 其他層是從較淺的學習層複製的
> 模型。

這個構造的解決方案的存在表明
更深的模型應該不會產生更高的訓練誤差
比其較淺的對應物。



在本文中，我們通過以下方式解決退化問題
<u>引入深度殘差學習框架。 而不是希望每幾個堆疊層直接適合一個所需的底層映射</u>，我們明確地讓這些層適合殘差映射。 形式上，表示所需的
底層映射為 H(x)，我們讓堆疊的非線性
層擬合 F(x) := H(x)−x 的另一個映射。 原始映射被重鑄為 F(x)+x。 我們假設它
優化殘差映射比優化更容易
原始的、未引用的映射。 在極端情況下，如果一個
身份映射是最優的，它會更容易推送
殘差為零而不是通過堆棧擬合身份映射非線性層。



# 引用

- https://arxiv.org/pdf/1512.03385.pdf