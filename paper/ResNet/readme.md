# ResNet论文记录和代码复现

## Abstract

深層次的神經網路難訓練，提出殘差學習框架來簡化訓練，



之前。 我們明確地將層重新表述為參考層輸入的學習殘差函數，而不是學習未參考函數。







## Introduction

深度網絡以端到端的多層方式自然地集成了低/中/高級特徵[50]和分類器，並且可以豐富特徵的“層次”
按堆疊層數（深度）



深度學習發展過程中，出現了一個問題：是
學習更好的網絡就像堆疊更多層一樣簡單？
回答這個問題的一個障礙是臭名昭著的
**梯度消失/爆炸問題**



這個問題，從一開始就阻礙了收斂，然而這個問題在很大程度上通過歸一化初始化和中間歸一化得到數，層網路開始收斂，隨機梯度開始收斂。



當更深層次的網絡能夠開始收斂時，退化問題已經暴露：隨著網絡層數的增加，準確率開始變得飽和，



> （訓練準確性的）退化表明沒有所有系統都同樣易於優化。 讓我們考慮一個
> 較淺的架構及其更深的對應物
> 更多層在上面。 存在構造解
> 到更深的模型：添加的層是恆等映射，
> 其他層是從較淺的學習層複製的
> 模型。

這個構造的解決方案的存在表明
更深的模型應該不會產生更高的訓練誤差
比其較淺的對應物。



在本文中，我們通過以下方式解決退化問題
<u>引入深度殘差學習框架。 而不是希望每幾個堆疊層直接適合一個所需的底層映射</u>，我們明確地讓這些層適合殘差映射。 形式上，表示所需的
底層映射為 H(x)，我們讓堆疊的非線性
層擬合 F(x) := H(x)−x 的另一個映射。 原始映射被重鑄為 F(x)+x。 我們假設它
優化殘差映射比優化更容易
原始的、未引用的映射。 在極端情況下，如果一個
身份映射是最優的，它會更容易推送
殘差為零而不是通過堆棧擬合身份映射非線性層。



我們在 ImageNet 上進行了全面的實驗
[36] 展示退化問題並評估我們的
方法。 我們表明：1）我們極深的殘差網絡
很容易優化，但是對應的“普通”網絡（即
簡單地堆疊層）表現出更高的訓練錯誤時
深度增加； 2）我們的深度殘差網可以輕鬆享受
大大增加的深度提高了準確性，產生的結果比以前的網絡好得多。

## Related Work

 在低級視覺和計算機圖形學中，為了求解偏微分方程 (PDE)，廣泛使用
多重網格方法 [3] 將系統重新表述為多個尺度的子問題，其中每個子問題負責較粗和較細之間的殘差解
規模。 Multigrid 的替代方法是分層基礎預處理 [45, 46]，它依賴於表示兩個尺度之間的殘差向量的變量。 它已經顯示
[3, 45, 46] 這些求解器比不知道剩餘性質的標準求解器收斂得更快解決方案。 

<u>這些方法表明，一個好的重新制定或預處理可以簡化優化。</u>



## Deep Residual learning

這種重新表述的動機是違反直覺的
關於退化問題的現象（圖 1，左）。 作為
我們在介紹中討論過，如果添加的層可以
被構造為身份映射，一個更深的模型應該
訓練誤差不大於較淺的對應物。 退化問題表明求解器
可能難以近似恆等映射
由多個非線性層。 通過殘差學習重構，如果身份映射是最優的，求解器
可以簡單地將多個非線性層的權重推向零以接近恆等映射。

在實際情況下，恆等映射不太可能是最優的，但我們的重新表述可能有助於預先設定
問題。 如果最優函數更接近身份
映射比到零映射，它應該更容易
求解器找到參考身份的擾動
映射，而不是像學習新功能一樣學習功能。 我們展示
通過實驗（圖 7），學習到的殘差函數在
general 的反應很小，表明身份映射提供了合理的預處理。





## Experiments

我們認為這種優化難度不太可能
由梯度消失引起。 這些普通網絡是
用 BN [16] 訓練，<u>確保前向傳播</u>
<u>信號具有非零方差</u>。 我們還驗證了
向後傳播的梯度表現出健康的規範
國陣。 所以前向或後向信號都不會消失。 在
事實上，34 層普通網絡仍然能夠達到具有競爭力的精度（表 3），表明求解器有效
在某種程度上。 我們推測深平網可能
具有指數級的低收斂率，這會影響
減少訓練誤差3
. 這種優化困難的原因將在未來進行研究。





# 代碼實現

<img src= 'https://production-media.paperswithcode.com/methods/Screen_Shot_2020-09-25_at_10.26.40_AM_SAB79fQ.png'>



- pytorch

  https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py

  

# 引用

- https://arxiv.org/pdf/1512.03385.pdf

  

- https://www.geeksforgeeks.org/residual-networks-resnet-deep-learning/

- 